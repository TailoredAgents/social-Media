# Ensure warnings are suppressed in worker processes
from backend.core.suppress_warnings import suppress_third_party_warnings
suppress_third_party_warnings()

from backend.tasks.celery_app import celery_app
from backend.agents.tools import openai_tool, memory_tool
import logging
from datetime import datetime, timedelta
import random

logger = logging.getLogger(__name__)

@celery_app.task
def analyze_performance():
    """Analyze overall content performance and provide optimization recommendations"""
    try:
        # Simulate performance data (in production, this would come from social media APIs)
        performance_data = {
            'total_posts': 156,
            'avg_engagement_rate': 4.2,
            'top_performing_posts': [
                {
                    'platform': 'LinkedIn',
                    'content': 'AI is transforming how we approach content marketing...',
                    'engagement_rate': 8.5,
                    'reach': 5200,
                    'published_at': '2025-07-20T10:00:00Z'
                },
                {
                    'platform': 'Twitter',
                    'content': '5 quick tips for better social media engagement...',
                    'engagement_rate': 6.8,
                    'reach': 3400,
                    'published_at': '2025-07-21T15:00:00Z'
                }
            ],
            'low_performing_posts': [
                {
                    'platform': 'Instagram',
                    'content': 'Generic motivational quote with no context',
                    'engagement_rate': 1.2,
                    'reach': 450,
                    'published_at': '2025-07-19T12:00:00Z'
                }
            ],
            'platform_performance': {
                'twitter': {'avg_engagement': 3.8, 'total_posts': 45},
                'linkedin': {'avg_engagement': 5.1, 'total_posts': 32},
                'instagram': {'avg_engagement': 2.9, 'total_posts': 25}
            }
        }
        
        # Generate optimization recommendations
        optimization_prompt = f"""Analyze this social media performance data and provide specific optimization recommendations:

Performance Data:
- Total Posts: {performance_data['total_posts']}
- Average Engagement Rate: {performance_data['avg_engagement_rate']}%
- Platform Performance: {performance_data['platform_performance']}

Top Performing Content:
{[post['content'][:100] + '...' for post in performance_data['top_performing_posts']]}

Low Performing Content:
{[post['content'][:100] + '...' for post in performance_data['low_performing_posts']]}

Provide:
1. Key performance insights
2. Content optimization recommendations
3. Platform-specific strategies
4. Posting time and frequency suggestions
5. Content format recommendations

Focus on actionable, data-driven recommendations."""

        analysis = openai_tool.generate_text(optimization_prompt, max_tokens=500)
        
        # Store analysis in memory
        memory_tool.store_content(
            content=analysis,
            metadata={
                'type': 'performance_analysis',
                'analysis_date': datetime.utcnow().isoformat(),
                'total_posts_analyzed': performance_data['total_posts']
            }
        )
        
        return {
            'status': 'success',
            'message': 'Performance analysis completed',
            'performance_data': performance_data,
            'recommendations': analysis,
            'analysis_date': datetime.utcnow().isoformat()
        }
        
    except Exception as exc:
        logger.error(f"Performance analysis failed: {str(exc)}")
        return {
            'status': 'error',
            'message': f'Performance analysis failed: {str(exc)}'
        }

@celery_app.task
def optimize_posting_schedule(user_id, platform=None):
    """Optimize posting schedule based on audience engagement patterns"""
    try:
        # Simulate engagement data by time/day (in production, get from analytics APIs)
        engagement_patterns = {
            'twitter': {
                'monday': {'09:00': 5.2, '12:00': 6.8, '15:00': 4.9, '17:00': 7.1},
                'tuesday': {'09:00': 4.8, '12:00': 6.2, '15:00': 5.5, '17:00': 6.9},
                'wednesday': {'09:00': 5.5, '12:00': 7.2, '15:00': 5.8, '17:00': 7.4},
                'thursday': {'09:00': 5.1, '12:00': 6.9, '15:00': 5.2, '17:00': 7.8},
                'friday': {'09:00': 4.2, '12:00': 5.8, '15:00': 4.5, '17:00': 6.2},
            },
            'linkedin': {
                'monday': {'08:00': 6.8, '10:00': 8.2, '12:00': 7.5, '17:00': 9.1},
                'tuesday': {'08:00': 7.2, '10:00': 8.9, '12:00': 8.1, '17:00': 9.5},
                'wednesday': {'08:00': 7.8, '10:00': 9.2, '12:00': 8.7, '17:00': 9.8},
                'thursday': {'08:00': 7.5, '10:00': 8.8, '12:00': 8.3, '17:00': 9.3},
                'friday': {'08:00': 6.9, '10:00': 7.8, '12:00': 7.2, '17:00': 8.1},
            }
        }
        
        recommendations = {}
        platforms_to_analyze = [platform] if platform else ['twitter', 'linkedin', 'instagram']
        
        for p in platforms_to_analyze:
            if p in engagement_patterns:
                platform_data = engagement_patterns[p]
                
                # Find optimal times
                best_times = {}
                for day, times in platform_data.items():
                    best_time = max(times.items(), key=lambda x: x[1])
                    best_times[day] = {
                        'time': best_time[0],
                        'engagement_rate': best_time[1]
                    }
                
                recommendations[p] = {
                    'optimal_schedule': best_times,
                    'recommended_frequency': '1-2 posts per day' if p == 'twitter' else '3-5 posts per week',
                    'best_overall_time': max(
                        [(day, data['time'], data['engagement_rate']) 
                         for day, data in best_times.items()],
                        key=lambda x: x[2]
                    )
                }
        
        optimization_summary = f"""Posting schedule optimization completed for user {user_id}.
        
Recommendations:
{recommendations}

Key insights:
- LinkedIn performs best on Wednesday mornings
- Twitter engagement peaks on Thursday evenings
- Consistent posting at optimal times can increase engagement by 15-25%
"""
        
        # Store recommendations
        memory_tool.store_content(
            content=optimization_summary,
            metadata={
                'type': 'schedule_optimization',
                'user_id': user_id,
                'platform': platform or 'all',
                'optimization_date': datetime.utcnow().isoformat()
            }
        )
        
        return {
            'status': 'success',
            'message': 'Posting schedule optimized',
            'user_id': user_id,
            'recommendations': recommendations,
            'summary': optimization_summary
        }
        
    except Exception as exc:
        logger.error(f"Schedule optimization failed: {str(exc)}")
        return {
            'status': 'error',
            'message': f'Schedule optimization failed: {str(exc)}'
        }

@celery_app.task
def a_b_test_content(content_variants, platform):
    """Run A/B tests on different content variations"""
    try:
        if len(content_variants) < 2:
            return {
                'status': 'error',
                'message': 'At least 2 content variants required for A/B testing'
            }
        
        # Simulate A/B test results (in production, this would track real engagement)
        results = []
        
        for i, variant in enumerate(content_variants):
            # Simulate engagement metrics
            engagement_rate = round(random.uniform(2.0, 8.0), 2)
            reach = random.randint(500, 5000)
            clicks = random.randint(10, 200)
            
            result = {
                'variant': chr(65 + i),  # A, B, C, etc.
                'content': variant,
                'engagement_rate': engagement_rate,
                'reach': reach,
                'clicks': clicks,
                'click_through_rate': round((clicks / reach) * 100, 2)
            }
            
            results.append(result)
        
        # Determine winner
        winner = max(results, key=lambda x: x['engagement_rate'])
        
        # Generate insights
        insights_prompt = f"""Analyze these A/B test results for {platform} and provide insights:

Test Results:
{results}

Winner: Variant {winner['variant']} with {winner['engagement_rate']}% engagement

Provide:
1. Why the winning variant likely performed better
2. Key differences between variants
3. Recommendations for future content
4. Elements to test next

Content Variants:
{[f"Variant {chr(65 + i)}: {variant[:100]}..." for i, variant in enumerate(content_variants)]}
"""
        
        insights = openai_tool.generate_text(insights_prompt, max_tokens=400)
        
        # Store A/B test results
        memory_tool.store_content(
            content=insights,
            metadata={
                'type': 'ab_test_results',
                'platform': platform,
                'winner_variant': winner['variant'],
                'test_date': datetime.utcnow().isoformat()
            }
        )
        
        return {
            'status': 'success',
            'message': f'A/B test completed for {len(content_variants)} variants',
            'platform': platform,
            'results': results,
            'winner': winner,
            'insights': insights,
            'test_date': datetime.utcnow().isoformat()
        }
        
    except Exception as exc:
        logger.error(f"A/B testing failed: {str(exc)}")
        return {
            'status': 'error',
            'message': f'A/B testing failed: {str(exc)}'
        }

@celery_app.task
def generate_performance_report(date_range_days=30):
    """Generate comprehensive performance report"""
    try:
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=date_range_days)
        
        # Simulate comprehensive performance data
        report_data = {
            'date_range': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': date_range_days
            },
            'overall_metrics': {
                'total_posts': 156,
                'total_engagement': 2847,
                'avg_engagement_rate': 4.2,
                'total_reach': 45600,
                'new_followers': 342,
                'content_pieces': 89
            },
            'platform_breakdown': {
                'twitter': {
                    'posts': 45, 'engagement_rate': 3.8, 'reach': 12500,
                    'best_post': 'AI trends thread with 150+ likes'
                },
                'linkedin': {
                    'posts': 32, 'engagement_rate': 5.1, 'reach': 18900,
                    'best_post': 'Industry analysis with 89 comments'
                },
                'instagram': {
                    'posts': 25, 'engagement_rate': 6.2, 'reach': 14200,
                    'best_post': 'Behind-the-scenes video with 200+ likes'
                }
            },
            'content_performance': {
                'top_topics': ['AI & Technology', 'Marketing Tips', 'Industry News'],
                'best_formats': ['Educational carousels', 'Quick tips', 'Industry insights'],
                'optimal_times': {
                    'twitter': 'Thursday 5-6 PM',
                    'linkedin': 'Wednesday 10-11 AM',
                    'instagram': 'Tuesday 1-2 PM'
                }
            }
        }
        
        # Generate executive summary
        summary_prompt = f"""Create an executive summary for this social media performance report:

Date Range: {date_range_days} days
Performance Data: {report_data}

Provide:
1. Key achievements and highlights
2. Areas for improvement
3. Strategic recommendations
4. Next steps and goals

Keep it concise but comprehensive for executive review."""

        executive_summary = openai_tool.generate_text(summary_prompt, max_tokens=400)
        
        report = {
            'report_id': f"report_{int(datetime.utcnow().timestamp())}",
            'generated_at': datetime.utcnow().isoformat(),
            'date_range': report_data['date_range'],
            'executive_summary': executive_summary,
            'metrics': report_data['overall_metrics'],
            'platform_breakdown': report_data['platform_breakdown'],
            'content_insights': report_data['content_performance'],
            'recommendations': [
                'Increase LinkedIn posting frequency to 5x/week',
                'Test video content on Instagram Stories',
                'Create more educational carousel posts',
                'Optimize posting times based on engagement data'
            ]
        }
        
        # Store report
        memory_tool.store_content(
            content=str(report),
            metadata={
                'type': 'performance_report',
                'report_id': report['report_id'],
                'date_range_days': date_range_days
            }
        )
        
        return {
            'status': 'success',
            'message': f'Performance report generated for {date_range_days} days',
            'report': report
        }
        
    except Exception as exc:
        logger.error(f"Performance report generation failed: {str(exc)}")
        return {
            'status': 'error',
            'message': f'Performance report generation failed: {str(exc)}'
        }